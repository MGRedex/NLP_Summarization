{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RedeX\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchinfo\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import (\n",
    "    DataLoader,\n",
    "    SequentialSampler,\n",
    "    BatchSampler,\n",
    "    RandomSampler\n",
    ")\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ExponentialLR, CyclicLR\n",
    "from torcheval import metrics\n",
    "from torch.profiler import (\n",
    "    profile,\n",
    "    ProfilerActivity,\n",
    "    tensorboard_trace_handler\n",
    ")\n",
    "import datasets\n",
    "from datasets import (\n",
    "    Features,\n",
    "    Array2D\n",
    ")\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertTokenizer\n",
    ")\n",
    "import os\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from typing import (\n",
    "    Optional,\n",
    "    Dict \n",
    ")\n",
    "import lightning as L\n",
    "\n",
    "import pandas as pd\n",
    "from src import data, neural, func\n",
    "\n",
    "try:\n",
    "    from config import config\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader, validation_dataloader = data.create_dataloaders(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test different configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func.test_model(config, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(20 + 16 + 16)\n",
    "print((48.5 + 60.1 + 60.6) / 3)\n",
    "print((0.006 + 0.006 + 0.005) / 3)\n",
    "print(f'''\n",
    "WORKERS: {config[\"DATA\"][\"DATALOADER_NUM_WORKERS\"]};\n",
    "NON_BLOCKING: {config[\"DATA\"][\"NON_BLOCKING\"]};\n",
    "DTYPE: {config[\"MODEL\"][\"DTYPE\"]};\n",
    "WARMUP: {config[\"MODEL\"][\"WARMUP\"]};\n",
    "TF32: {config[\"MODEL\"][\"TF32\"]}'''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_tokenizer.add_special_tokens({\n",
    "    \"eos_token\": \"[EOS]\",\n",
    "    \"bos_token\": \"[BOS]\"\n",
    "})\n",
    "\n",
    "model = neural.Transformer(\n",
    "    vocab_size = len(bert_tokenizer),\n",
    "    seq_len = config[\"MODEL\"][\"SEQ_LEN\"],\n",
    "    emb_dim = config[\"MODEL\"][\"EMB_DIM\"],\n",
    "    n_heads = config[\"MODEL\"][\"ATTN_HEADS\"],\n",
    "    feedforward_dim = config[\"MODEL\"][\"FF_DIM\"],\n",
    "    dropouts = config[\"MODEL\"][\"DROPOUTS\"],\n",
    "    dtype = config[\"MODEL\"][\"DTYPE\"],\n",
    "    activation = nn.LeakyReLU(),\n",
    "    enc_num = 5,\n",
    "    dec_num = 5,\n",
    ").to(config[\"DEVICE\"])\n",
    "\n",
    "writer = func.create_run_logger(config[\"TRAINING\"][\"LOGS_FOLDER\"], model)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "optimizer = Adam(params = model.parameters(), betas = (0.9, 0.999), eps = 1e-8)\n",
    "model_metrics = {\"BLEU\": metrics.BLEUScore(n_gram = 3)}\n",
    "scheduler = CyclicLR(optimizer, base_lr = 1e-4, max_lr = 1e-3, step_size_up = 10, cycle_momentum = False)\n",
    "\n",
    "trainer = neural.Trainer(\n",
    "    model = model,\n",
    "    optimizer = optimizer,\n",
    "    loss_fn = loss_fn,\n",
    "    scheduler = scheduler,\n",
    "    tokenizer = bert_tokenizer,\n",
    "    epoch = config[\"TRAINING\"][\"EPOCH\"],\n",
    "    device = config[\"DEVICE\"],\n",
    "    checkpoint_path = config[\"TRAINING\"][\"CHECKPOINT_PATH\"],\n",
    "    checkpoint_by = config[\"TRAINING\"][\"CHECKPOINT_BY\"],\n",
    "    non_blocking = config[\"DATA\"][\"NON_BLOCKING\"],\n",
    "    warmup = config[\"MODEL\"][\"WARMUP\"],\n",
    "    max_src_len = 191,\n",
    "    max_tgt_len = 80,\n",
    "    batch_size = config[\"DATA\"][\"BATCH_SIZE\"],\n",
    "    model_metrics = model_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"train_states/best_state_5dec.pt\")\n",
    "model.state_dict().update(state_dict[\"model_state\"])\n",
    "scheduler.state_dict().update(state_dict[\"scheduler_state\"])\n",
    "trainer.BLEU = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load_state_dict(torch.load(config[\"TRAINING\"][\"CHECKPOINT_PATH\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(train_dataloader, validation_dataloader, validation_rate = 2, writer = writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.validate(validation_dataloader, model_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(validation_dataloader))\n",
    "index = 2\n",
    "# model(sample[\"document\"][index].unsqueeze(0).to(config[\"DEVICE\"]), sample[\"summary\"][index].unsqueeze(0).to(config[\"DEVICE\"])).shape\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    pred = model(sample[\"document\"].to(config[\"DEVICE\"]), sample[\"summary\"].to(config[\"DEVICE\"])).argmax(-1)\n",
    "bert_tokenizer.batch_decode(pred, skip_special_tokens = True)[index], bert_tokenizer.decode(sample[\"summary\"][index], skip_special_tokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    x, y = batch[\"document\"].to(config[\"DEVICE\"], non_blocking = config[\"DATA\"][\"NON_BLOCKING\"]), batch[\"summary\"].to(config[\"DEVICE\"], non_blocking = config[\"DATA\"][\"NON_BLOCKING\"])\n",
    "\n",
    "    y_pred = model(x, y)\n",
    "\n",
    "    summary_with_eos = F.pad(y[:, 1:], pad = (0,1), value = 0)\n",
    "    summary_with_eos[torch.arange(config[\"DATA\"][\"BATCH_SIZE\"], device = config[\"DEVICE\"]), summary_with_eos.argmin(dim = 1)] = bert_tokenizer.eos_token_id\n",
    "\n",
    "    loss = loss_fn(y_pred.view(-1, y_pred.shape[2]), summary_with_eos.view(-1))\n",
    "    loss.backward()\n",
    "    ave_grads = []\n",
    "    max_grads= []\n",
    "    layers = []\n",
    "    for n, p in model.named_parameters():\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean().cpu())\n",
    "            max_grads.append(p.grad.abs().max().cpu())\n",
    "    writer.add_scalars(\"test_grads\",{name: value for name,value in zip(layers, ave_grads)}, step)\n",
    "    optimizer.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
